name: 'SimpleEN_FR'
            
training: 
  stochastic: True
  prop_gold: 0.
  denoising: True
  adversial: False
  lm_pretraining: True
  share_enc: 1 #Number of layers to share in the encoders
  share_dec: 1 #Number of layers to share in the decoders
  noise_function: 'shuffle_drop_blank'
  noise_intensity:
    word_shuffle: 3
    word_dropout: 0.1
    word_blank: 0.2
  adversial_patience: 1
  batch_size: 32
  lr: 0.001
  lr adversial: 0.001
  n_epochs: 150
  pretraining_n_epochs: 20
  analyse_every: 50
  wandb: False
  early_stopping: False
  early_stopping_patience: 5

dataset:
  max_len: 64
  only_gold_translation: True # If we only have acces to a gold translation (not the gold encoder/decoder)
  language_similarity: 'bleu'  
  joint_tokenization: True
  size: 100000
  vocab_size: 10000
  pad_index: 0
  bos1_index: 1
  bos2_index: 2
  eos_index: 3
  L:
    param: None
  L1:
    name: 'en'
  L2:
    name: 'fr'
  M:
    param: None


models:
  encoder:
    model: "lstm"
    batch_norm: False
    lstm:
      share_lang_emb: True # Share embedding layers between languages (enc / dec / proj)
      emb_dim: 512
      hidden_dim: 512
      dropout: 0.
      n_enc_layers: 1
      enc_dim: 512 # Latent space dimension
      proj_mode: 'last' # Projection mode (proj / pool / last)
      freeze_enc_emb: False 
  decoder:
    model: "lstm"
    lstm:
      share_lang_emb: True # Share embedding layers between languages (enc / dec / proj)
      share_encdec_emb: True  # Share encoder embeddings / decoder embeddings
      share_decpro_emb: False # Share decoder embeddings / decoder output projection
      share_output_emb: True # Share decoder output embeddings
      share_lstm_proj: False # Share projection layer between decoder LSTM and output layer)
      emb_dim: 512
      hidden_dim: 512
      lstm_proj: False # Projection layer between decoder LSTM and output layer
      dropout: 0.
      n_dec_layers: 1
      enc_dim: 512
      init_encoded: False # Initialize the decoder with the encoded state. Append it to each input embedding otherwise.
      freeze_dec_emb: False 
  discriminator:
    model: "mlp"
    mlp:
      dim_in: 2
      dim_hidden: 3
      dim_out: 1

  
            
            

      